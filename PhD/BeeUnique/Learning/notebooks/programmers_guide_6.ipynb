{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured Text\n",
    "Most of the data we've dealt with so far has been structured. Unstructured data involves things like emails, blog posts, articles etc. One useful thing you can do with this data is positive/negative sentiment analysis.\n",
    "\n",
    "If we want to decide if someone likes or dislikes different foods, we can come up with a list of words that provide evidence the person would like/dislike the food:\n",
    "\n",
    "| Like  | Dislike  |\n",
    "|:-:|:-:|\n",
    "| Delicious  | Awful  |\n",
    "| Tasty  | Bland  |\n",
    "| Good  | Bad  |\n",
    "| Love  | Hate  |\n",
    "| Smooth  | Gritty  |\n",
    "\n",
    "We can count how many like and dislike words there are are and use that to classify the text. Rather than raw counts though, you can use naive Bayes:\n",
    "\n",
    "$$\n",
    "h_{MAP} = arg max_{h \\in H} P(D|h)P(h)\n",
    "$$\n",
    "\n",
    "- $h_{MAP} = arg max_{h \\in H}$: For each of the hypotheses, find the one with max probability\n",
    "- $P(D|h)$ = Probability of data given the hypothesis (e.g. probability of seeing specific words in the text given text)\n",
    "- $P(h)$ = Probability of the hypothesis\n",
    "\n",
    "We can have a training dataset of text called the training __corpus__. Each entry in the corpus is a __document__ (e.g. Twitter tweet). Each document is labelled a class: e.g. favourable/unfavourable and we train our classifier using this corpus.\n",
    "\n",
    "- P(Favourable) = 0.5\n",
    "- P(Unfavourable) = 0.5\n",
    "\n",
    "$P(D|h)$ is the probability of seeing some evidence (data) given the hypothesis h. The data is the text. We can calculate the probability for each word in all the text, but there's so many words it could take a while. So instead we can treat documents like bags of words.\n",
    "\n",
    "Instead of calculating if the third word is 'thrill' in a favourable review, you ask what's the probability that the word 'thrill' occurs in a favourable document.\n",
    "\n",
    "## Training\n",
    "First we determine the __vocabulary__ (unique words) of all the documents in the corpus. |Vocabulary| = number of words ni the vocabulary. Next, for word $w_k$ in the vocabulary, we compute the probability of that word occurring given each hypothesis: $P(w_k|h_i)$\n",
    "\n",
    "1. Combine documents tagged with hypothesis into a single text file\n",
    "2. Count how many word occurrences there are in the file. e.g. if there are 500 words, n = 500\n",
    "3. For each word in the vocabulary $w_k$, count how many times that word occurred in the text. Call this $n_k$\n",
    "4. For each word in vocabulary, compute:\n",
    "\n",
    "$$\n",
    "P(w_k|h_i) = \\frac{n_k + 1}\n",
    "{n + |vocabulary|}\n",
    "$$\n",
    "\n",
    "Once the training phrase is complete, we can classify documents with the formula shown above:\n",
    "\n",
    "$$\n",
    "h_{MAP} = arg max_{h \\in H} P(D|h)P(h)\n",
    "$$\n",
    "\n",
    "## After Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P(word|dislike)</th>\n",
       "      <th>P(word|like)</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>gravity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00070</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.00070</td>\n",
       "      <td>hype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00470</td>\n",
       "      <td>0.00500</td>\n",
       "      <td>over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>stunned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.04650</td>\n",
       "      <td>0.04700</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   P(word|dislike)  P(word|like)     word\n",
       "0          0.00900       0.00700       am\n",
       "1          0.01200       0.01200       by\n",
       "2          0.00050       0.00200     good\n",
       "3          0.00001       0.00001  gravity\n",
       "4          0.00070       0.00300    great\n",
       "5          0.00200       0.00070     hype\n",
       "6          0.01000       0.01000        I\n",
       "7          0.00470       0.00500     over\n",
       "8          0.00200       0.00090  stunned\n",
       "9          0.04650       0.04700      the"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "test = ['I', 'am', 'stunned', 'by', 'the', 'hype', 'over', 'gravity']\n",
    "words = {   'word': ['am', 'by', 'good', 'gravity', 'great', 'hype', 'I', 'over', 'stunned', 'the'],\n",
    "            'P(word|like)': [0.007, 0.012, 0.002, 0.00001, 0.003, 0.0007, 0.01, 0.005, 0.0009, 0.047],\n",
    "            'P(word|dislike)': [0.009, 0.012, 0.0005, 0.00001, 0.0007, 0.002, 0.01, 0.0047, 0.002, 0.0465],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(words)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting test tweet\n",
    "- P(like) x P(I|like) x P(am|like) ...\n",
    "- P(dislike) x P(I|dislike) x P(am|dislike) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2181e-22 4.72068e-21\n",
      "Higher probability that the tweet should be classified in the dislike category\n"
     ]
    }
   ],
   "source": [
    "def like_vs_dislike(tokens, probabilities_df, h_liked_p, h_disliked_p):\n",
    "    total_liked_p = h_liked_p\n",
    "    total_disliked_p = h_disliked_p\n",
    "    for t in tokens:\n",
    "        token_probabilities = probabilities_df[probabilities_df['word'] == t]\n",
    "        liked_p = token_probabilities['P(word|like)'].values[0]\n",
    "        disliked_p = token_probabilities['P(word|dislike)'].values[0]\n",
    "        \n",
    "        total_liked_p *= liked_p\n",
    "        total_disliked_p *= disliked_p\n",
    "        \n",
    "    return (total_liked_p, total_disliked_p)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "like_p, dislike_p = like_vs_dislike(test, df, 0.5, 0.5)\n",
    "print(like_p, dislike_p)\n",
    "print('Higher probability that the tweet should be classified in the dislike category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Numbers and Python\n",
    "Word probabilities are very small numbers, and multiplying them gives us a tiny number - Python has trouble with small numbers and can truncate them to 0s. To solve this we can add the logs of the probabilities rather than multiplying the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-6.907755278982137\n"
     ]
    }
   ],
   "source": [
    "print(0.0001 ** 100)\n",
    "print(0 + math.log(0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithms\n",
    "$b^n=x$ The log of a number (x) is the exponent (n) that you need to raise a base (b) to equal the number.\n",
    "\n",
    "$log_{10}(1000) = 3$ since 1000 = $10^3$\n",
    "\n",
    "Base of the Python log function is _e_. Log allows us to compress the scale of a number although instead of multiplying probabilities, we add the logs of the probabilities.\n",
    "\n",
    "## Stop Words\n",
    "You can often throw out the 200 most frequent words in the English language as they don't tend to add any useful information to the classifier. That said, sometimes using the most frequent words and throwing out the rest can be useful for certain tasks (such as identifying the time period when documents were written). In online chats, predators can also use the words 'I', 'me' and 'you' more frequently than other people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
