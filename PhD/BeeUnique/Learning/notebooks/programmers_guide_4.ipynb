{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Algorithms\n",
    "How do you determine how accurate a classifier is and how it compares to others?\n",
    "\n",
    "## Accuracy\n",
    "Most common technique is to generate a training and testing set of data - must be random.\n",
    "\n",
    "## 10-Fold Cross Validation\n",
    "Using this technique means we're using 90% of our data rather than 50% of it (if we split our data in half for training and testing).\n",
    "\n",
    "- Randomly divide data into 10 parts - e.g. if two classes and 1000 rows, each part has 50 of both classes\n",
    "- Iterate through the following steps 10 times\n",
    "- Use 9 for training and 1 for testing - change testing group each time\n",
    "- Test the classifier with data from testing group and record result\n",
    "- Finally, sum the results of how many were predicted correctly and incorrectly\n",
    "\n",
    "## Leave-One-Out\n",
    "Another name for n-fold cross validation, occurs when you leave one out each time from the data to test and train with the rest. So if you have 1000 datapoints, you train on 999 and test on 1, until every point has been tested.\n",
    "\n",
    "This technique lets us use the largest possible amount of data for training and is deterministic: since we're testing every value, we should always get the same result from our tests. The other techinques involve randomly splitting data up into training and testing groups so there's a level of chance in the result you get.\n",
    "\n",
    "Disadvantages\n",
    "- Computationally expensive to perform\n",
    "- Stratification: We want to have examples of all classes in our testing dataset. Can solve this with k-fold validation where you ensure classes are present in each testing bucket in the same proportion they're present in the overall dataset. Leave-one-out tests are non-stratified as you only have one class per test\n",
    "- Leave-one-out tends to be used if you have small datasets, otherwise k-fold is popular\n",
    "\n",
    "## Confusion Matrices\n",
    "Used to determine how the model works predicting the different classes in the dataset. Rows are the actual class of a predicted case and the columns are the predictions that were made for the different classes.\n",
    "\n",
    "## Kappa Statistics\n",
    "Compares performance of a classifier to another classifer that makes random predictions.\n",
    "\n",
    "## Rote Classifier\n",
    "Memorises training set and make a prediction if it finds a match when presented with new data.\n",
    "\n",
    "## Nearest Neighbour and Outliers\n",
    "Problems can occur with outliers in your data skewing predictions. To help with this, you can use multiple nearest neighbours to increase accuracy. This is called the voting method, as the class that occurs most frequently with the neighbours is the prediction that will be made. This works for discrete classes (as opposed to continous).\n",
    "\n",
    "You can weight the class of closer points more strongly than those further away. To do this, you can calculate the inverse of the distance (1 over the distance):\n",
    "\n",
    "| User  | Distance  |  Rating |\n",
    "|:---:|:---:|:---:|\n",
    "|  Sally | 5  |  4 |\n",
    "| Tara  |  10 |  5 |\n",
    "|  Jade |  15 | 5  |\n",
    "\n",
    "Inverse of Sally's distance: 1/5\n",
    "\n",
    "| User  | Inverse Distance  |  Rating |\n",
    "|:---:|:---:|:---:|\n",
    "|  Sally | 0.2  |  4 |\n",
    "| Tara  |  0.1 |  5 |\n",
    "|  Jade |  0.067 | 5  |\n",
    "\n",
    "Divide each inverse distance by the sum of inverse distances: total=1.\n",
    "\n",
    "| User  | Influence  |  Rating |\n",
    "|:---:|:---:|:---:|\n",
    "|  Sally | 0.545  |  4 |\n",
    "| Tara  |  0.272 |  5 |\n",
    "|  Jade |  0.183 | 5  |\n",
    "\n",
    "Predicted score: $0.545 * 4 + 0.272 * 5 + 0.183 * 5 = 4.455$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
