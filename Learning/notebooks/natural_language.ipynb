{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: NLTK\n",
    "## Seperating\n",
    "First step is to organise in some way, you can separate by paragraphs or sentences. If you break up sentences you can refer back to the paragraph it came back from as paragraphs tend to be logical groupings of ideas.\n",
    "\n",
    "## Tokenizing\n",
    "Work tokenizer seperates by words, sentence tokenizer separates by sentences.\n",
    "\n",
    "## Corpa\n",
    "Body of text based around similar things: e.g. medical journals, presidential speeches, anything in the English language.\n",
    "\n",
    "## Lexicon\n",
    "Words and their meanings: Investors use word differently than your average person\n",
    "\n",
    "### Download Datasets\n",
    "```\n",
    "import nltk\n",
    "nltk.download() # download datasets\n",
    "```\n",
    "Downloads nltk_data directory in home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = \"Hello Mr. Smith, how are you doing today? The weather is great and Python is awesome. The sky is pinkish-blue. You should not eat cardboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Sentence\n",
    "Could try to group parts by punctuation, but what about cases where you have 'Mr.'? NLTK handles this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue.', 'You should not eat cardboard']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', 'not', 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "Words that are filler words - things you don't need for your analysis like 'a', 'the', etc. In NLTK you can set the language for these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourself', 'she', 's', 'don', 'didn', 'have', 'mustn', 'other', 'ma', 'how', 'your', 'can', 'them', 'out', 'itself', 'then', 'wasn', 'should', 'our', 'ourselves', 'about', 'isn', 'themselves', 'm', 'where', 'him', 'nor', 'shan', 'just', 're', 'ain', 'any', 'on', 'this', 'herself', 'so', 'or', 'be', 'with', 'me', 'doesn', 'do', 'shouldn', 'my', 'yours', 'hers', 'no', 'at', 'of', 'against', 'why', 'too', 'it', 'who', 'whom', 'these', 'did', 'into', 'each', 'those', 'd', 'aren', 'am', 'had', 'has', 'for', 'once', 'y', 'before', 'because', 'very', 'a', 'there', 'which', 'that', 'now', 'what', 'himself', 've', 'were', 'having', 'again', 'll', 'if', 'such', 'by', 'hadn', 'below', 'but', 'few', 'after', 'more', 'does', 'all', 'to', 'they', 'until', 'during', 'ours', 'his', 'in', 'as', 'hasn', 'weren', 'their', 'under', 'some', 'will', 'been', 'between', 'up', 'wouldn', 'was', 'theirs', 'is', 'above', 'we', 'through', 'being', 'you', 't', 'haven', 'than', 'not', 'here', 'over', 'its', 'both', 'her', 'yourselves', 'down', 'while', 'the', 'same', 'off', 'when', 'an', 'doing', 'mightn', 'myself', 'he', 'o', 'most', 'won', 'i', 'couldn', 'are', 'and', 'only', 'needn', 'further', 'from', 'own'}\n"
     ]
    }
   ],
   "source": [
    "example_text = \"This is an example showing off stop word filtration\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filtration']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(example_text)\n",
    "filtered_sentence = [w for w in words if w not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Words\n",
    "Find words that have the same base meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "example_words = ['python', 'pythoner', 'pythoning', 'pythoned', 'pythonly']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "All\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "new_text = \"It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Tagging\n",
    "Classifying different words - noun, etc.\n",
    "\n",
    "**POS tag list:**\n",
    "- CC\tcoordinating conjunction\n",
    "- CD\tcardinal digit\n",
    "- DT\tdeterminer\n",
    "- EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "- FW\tforeign word\n",
    "- IN\tpreposition/subordinating conjunction\n",
    "- JJ\tadjective\t'big'\n",
    "- JJR\tadjective, comparative\t'bigger'\n",
    "- JJS\tadjective, superlative\t'biggest'\n",
    "- LS\tlist marker\t1)\n",
    "- MD\tmodal\tcould, will\n",
    "- NN\tnoun, singular 'desk'\n",
    "- NNS\tnoun plural\t'desks'\n",
    "- NNP\tproper noun, singular\t'Harrison'\n",
    "- NNPS\tproper noun, plural\t'Americans'\n",
    "- PDT\tpredeterminer\t'all the kids'\n",
    "- POS\tpossessive ending\tparent's\n",
    "- PRP\tpersonal pronoun\tI, he, she\n",
    "- PRP$\tpossessive pronoun\tmy, his, hers\n",
    "- RB\tadverb\tvery, silently,\n",
    "- RBR\tadverb, comparative\tbetter\n",
    "- RBS\tadverb, superlative\tbest\n",
    "- RP\tparticle\tgive up\n",
    "- TO\tto\tgo 'to' the store.\n",
    "- UH\tinterjection\terrrrrrrrm\n",
    "- VB\tverb, base form\ttake\n",
    "- VBD\tverb, past tense\ttook\n",
    "- VBG\tverb, gerund/present participle\ttaking\n",
    "- VBN\tverb, past participle\ttaken\n",
    "- VBP\tverb, sing. present, non-3d\ttake\n",
    "- VBZ\tverb, 3rd person sing. present\ttakes\n",
    "- WDT\twh-determiner\twhich\n",
    "- WP\twh-pronoun\twho, what\n",
    "- WP\\$\tpossessive wh-pronoun\twhose\n",
    "- WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "# trained tokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# break statement so not too much output\n",
    "try:\n",
    "    for i in tokenized:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        print(tagged)\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "After splitting text up into sentences/words and identifying the noun/named entity as the subject. Chunking is used to identify descriptive words around noun and group things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# break statement so not too much output\n",
    "try:\n",
    "    for i in tokenized:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "\n",
    "        # use regular expressions to identify any adverb\n",
    "        chunk_gram = r'''Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'''\n",
    "        chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "        chunked = chunk_parser.parse(tagged)\n",
    "        print(chunked)\n",
    "        #chunked.draw()\n",
    "        break\n",
    "\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinking\n",
    "\n",
    "You chink something from a chunk - it is the removal of something. You can say you want to chunk everything except for somet things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk\n",
      "    THE/NNP\n",
      "    UNION/NNP\n",
      "    January/NNP\n",
      "    31/CD\n",
      "    ,/,\n",
      "    2006/CD\n",
      "    THE/NNP\n",
      "    PRESIDENT/NNP\n",
      "    :/:\n",
      "    Thank/NNP\n",
      "    you/PRP)\n",
      "  all/DT\n",
      "  (Chunk ./.))\n"
     ]
    }
   ],
   "source": [
    "# break statement so not too much output\n",
    "try:\n",
    "    for i in tokenized:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "\n",
    "        # chunk everything\n",
    "        chunk_gram = r'''Chunk: {<.*>+}\n",
    "                    # things to keep out\n",
    "                    }<VB.?|IN|DT>+{'''\n",
    "        chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "        chunked = chunk_parser.parse(tagged)\n",
    "        print(chunked)\n",
    "        #chunked.draw()\n",
    "        break\n",
    "\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "### Type and Examples\n",
    "- ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "- PERSON - Eddy Bonte, President Obama\n",
    "- LOCATION - Murray River, Mount Everest\n",
    "- DATE - June, 2008-06-29\n",
    "- TIME - two fifty a m, 1:30 p.m.\n",
    "- MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "- PERCENT - twenty pct, 18.75 %\n",
    "- FACILITY - Washington Monument, Stonehenge\n",
    "- GPE - South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  PRESIDENT/NNP\n",
      "  (NE GEORGE/NNP)\n",
      "  W./NNP\n",
      "  BUSH/NNP\n",
      "  'S/POS\n",
      "  (NE ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  A/NNP\n",
      "  JOINT/NNP\n",
      "  SESSION/NNP\n",
      "  OF/IN\n",
      "  (NE THE/NNP)\n",
      "  (NE CONGRESS/NNP)\n",
      "  ON/NNP\n",
      "  THE/NNP\n",
      "  STATE/NNP\n",
      "  OF/IN\n",
      "  (NE THE/NNP UNION/NNP)\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  THE/NNP\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# break statement so not too much output\n",
    "try:\n",
    "    for i in tokenized:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        # binary=True will try group entity words together: \"white house\" rather than \"white\" and \"house\"\n",
    "        named_entity = nltk.ne_chunk(tagged, binary=True)\n",
    "        print(named_entity)\n",
    "        break\n",
    "\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "Similar to stemming, may not find the original word but instead a synonym with the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "# default argument for pos is noun\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "\n",
    "sample = gutenberg.raw('bible-kjv.txt')\n",
    "tok = sent_tokenize(sample)\n",
    "print(tok[5:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "Can use to look up synonyms, antonyms, definitions and contexts of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "# find synonyms for the word 'program'\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n",
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas())\n",
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good', 'dependable', 'near', 'sound', 'full', 'unspoiled', 'safe', 'respectable', 'estimable', 'well', 'serious', 'commodity', 'beneficial', 'skillful', 'soundly', 'honorable', 'proficient', 'in_effect', 'secure', 'thoroughly', 'right', 'undecomposed', 'goodness', 'expert', 'just', 'upright', 'honest', 'practiced', 'skilful', 'salutary', 'unspoilt', 'trade_good', 'dear', 'ripe', 'effective', 'in_force', 'adept'}\n",
      "{'badness', 'ill', 'bad', 'evilness', 'evil'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing words\n",
    "\n",
    "Can use the Wu and Palmer (WUP) method to identify semantic relatedness of words: compare the similarity of two words and their tenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "0.32\n",
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "w3 = wordnet.synset('cat.n.01')\n",
    "w4 = wordnet.synset('car.n.01')\n",
    "\n",
    "# 90% similar\n",
    "print(w1.wup_similarity(w2))\n",
    "print(w1.wup_similarity(w3))\n",
    "print(w1.wup_similarity(w4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "Can be used to classify text as being about politics/military or to identify the gender of the author. A common use of this technique is to identify spam email.\n",
    "\n",
    "NLTK has a movie reviews database in its corpus and we're going to try classify reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['anastasia', 'contains', 'something', 'that', 'has', 'been', 'lacking', 'from', 'all', 'of', 'the', 'recent', 'disney', 'releases', '.', '.', '.', '(', 'especially', 'hercules', ')', '.', '.', '.', 'emotion', '.', 'all', 'the', 'wacky', 'characters', 'voiced', 'by', 'celebrities', 'and', 'fantastically', 'animated', 'adventure', 'sequences', 'aren', \"'\", 't', 'going', 'to', 'hold', 'anyone', \"'\", 's', 'interest', 'unless', 'there', 'is', 'an', 'emotional', 'core', 'to', 'hold', 'it', 'all', 'together', '.', 'not', 'since', 'disney', \"'\", 's', 'beauty', '&', 'the', 'beast', 'has', 'there', 'been', 'such', 'a', 'compelling', 'animated', 'film', 'with', 'interesting', 'characters', 'and', 'drama', 'that', 'works', '.', 'the', 'story', 'of', 'the', 'romanov', 'family', ',', 'the', 'rulers', 'of', 'russia', ',', 'and', 'their', 'downfall', 'begins', 'the', 'film', '.', 'anastasia', ',', 'one', 'of', 'the', 'daughters', ',', 'narrowly', 'escapes', 'the', 'mad', 'monk', 'rasputin', '(', 'voiced', 'by', 'christopher', 'lloyd', ')', 'with', 'her', 'grandmother', '(', 'voiced', 'by', 'angela', 'landsbury', ')', '.', 'but', 'anastasia', 'gets', 'lost', ',', 'and', 'grows', 'up', 'with', 'no', 'memory', 'of', 'her', 'royal', 'origins', '.', 'the', 'grandmother', 'lives', 'in', 'paris', ',', 'hoping', 'to', 'someday', 'find', 'her', 'beloved', 'anastasia', ',', 'the', 'only', 'member', 'of', 'the', 'family', 'to', 'survive', 'the', 'russian', 'revolution', '.', 'a', 'young', 'woman', 'named', 'anya', '(', 'voiced', 'by', 'meg', 'ryan', ')', 'runs', 'across', 'dimitri', '(', 'john', 'cusack', ')', 'and', 'vlad', '(', 'kelsey', 'grammar', ')', ',', 'who', 'are', 'trying', 'to', 'put', 'together', 'a', 'scheme', 'to', 'create', 'an', 'anastasia', 'to', 'fool', 'the', 'grandmother', 'and', 'get', 'the', 'reward', 'money', '.', 'they', 'pick', 'anya', 'to', 'pose', 'as', 'their', 'princes', ',', 'and', 'of', 'course', ',', 'it', 'is', 'soon', 'revealed', 'that', 'anya', 'is', 'in', 'fact', 'the', 'lost', 'anastasia', '.', 'dimitri', 'realizes', 'that', 'he', 'can', 'never', 'win', 'her', 'love', 'as', 'he', 'is', 'a', 'commoner', ',', 'and', 'meanwhile', ',', 'rasputin', 'is', 'not', 'dead', ',', 'and', 'plots', 'his', 'revenge', 'against', 'the', 'last', 'romanov', '.', 'the', 'plot', 'is', 'actually', 'quite', 'well', 'structured', '.', 'there', 'are', 'a', 'lot', 'of', 'complexities', 'that', 'may', 'have', 'to', 'be', 'explained', 'to', 'the', 'kids', 'later', ',', 'but', 'it', 'moves', 'along', 'fast', 'enough', 'that', 'they', \"'\", 'll', 'never', 'have', 'time', 'to', 'be', 'bored', '.', 'what', 'this', 'really', 'means', 'is', 'that', 'the', 'adults', 'won', \"'\", 't', 'be', 'bored', 'by', 'it', 'either', '.', 'there', 'is', 'a', 'great', 'deal', 'of', 'banter', 'between', 'anya', 'and', 'dimitri', 'that', 'is', 'very', 'funny', ',', 'and', 'their', 'relationship', 'develops', 'quite', 'naturally', 'as', 'the', 'film', 'progresses', '.', 'there', 'is', ',', 'of', 'course', ',', 'lots', 'of', 'comedy', 'for', 'the', 'kids', ',', 'including', 'rasputin', \"'\", 's', 'pet', 'bat', ',', 'bartok', '(', 'hank', 'azaria', ')', ',', 'who', 'is', 'easily', 'the', 'funniest', 'thing', 'in', 'the', 'film', '.', 'however', ',', 'the', 'comical', 'scenes', 'never', 'distract', 'from', 'the', 'drama', ',', 'but', 'are', 'worked', 'into', 'the', 'plot', 'almost', 'seamlessly', '.', 'the', 'animation', 'is', 'gorgeous', '-', 'the', 'characters', 'seem', 'to', 'come', 'to', 'life', 'through', 'the', 'talented', 'animators', '.', 'not', 'since', 'beauty', '&', 'the', 'beast', 'have', 'animated', 'characters', 'had', 'so', 'much', 'life', 'to', 'them', '.', 'even', 'without', 'the', 'voices', ',', 'they', 'act', '.', 'the', 'musical', 'number', 'in', 'paris', 'is', 'a', 'show', '-', 'stopper', ',', 'with', 'some', 'of', 'the', 'backgrounds', 'rendered', 'in', 'an', 'impressionist', 'painting', 'style', '.', 'as', 'with', 'all', 'recent', 'animated', 'features', ',', 'there', 'are', 'songs', ',', 'however', ',', 'these', 'songs', 'do', 'more', 'than', 'just', 'provide', 'fodder', 'for', 'top', '-', '40', 'singers', 'to', 'get', 'on', 'the', 'radio', '(', 'although', 'there', 'are', 'three', 'of', 'them', 'during', 'the', 'end', 'credits', ')', '.', 'the', 'songs', 'are', 'all', 'very', 'catchy', ',', 'and', 'advance', 'the', 'plot', ',', 'instead', 'of', 'just', 'being', 'showpieces', '(', 'except', 'for', 'the', 'paris', 'number', ',', 'but', 'that', \"'\", 's', 'so', 'much', 'fun', ',', 'it', \"'\", 's', 'okay', '.', '.', '.', ')', '.', 'i', 'can', \"'\", 't', 'reccomend', 'anastasia', 'highly', 'enough', '.', 'it', \"'\", 's', 'a', 'wonderful', 'film', 'that', 'ranks', 'right', 'up', 'there', 'with', 'other', 'animated', 'classics', '.', 'kids', 'and', 'adults', 'alike', 'will', 'enjoy', 'it', ',', 'and', 'it', \"'\", 's', 'also', 'nice', 'to', 'have', 'a', 'quality', 'animated', 'feature', 'film', 'from', 'another', 'studio', 'besides', 'disney', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# get tuple with words and pos/neg category\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "# shuffle as reviews are grouped by category\n",
    "random.shuffle(documents)\n",
    "\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Distribution\n",
    "\n",
    "Ordered from most common words through to least common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n",
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(len(all_words))\n",
    "print(all_words.most_common(15))\n",
    "print(all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stayin', 'heigh', 'faceless', 'slight', 'brooms', 'salads', 'galahad', 'liaison', 'laps', 'brethren', 'degenerated', '128', 'humility', 'osmet', 'fascination', 'homicides', 'scrambles', 'moonshiner', 'cross', 'shrinking']\n"
     ]
    }
   ],
   "source": [
    "# avoid using keys as it is not sorted\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "#word_features = all_words.most_common(10000)\n",
    "print(word_features[:20])\n",
    "#word_features = word_features[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        # return true/false if word present\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "# run example review\n",
    "example = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "print(example['laps'], example['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "#print(feature_sets[0][0]['swept'])\n",
    "#print(feature_sets[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 962, 'neg': 938}\n",
      "{'pos': 38, 'neg': 62}\n"
     ]
    }
   ],
   "source": [
    "training_set = feature_sets[:1900]\n",
    "testing_set = feature_sets[1900:]\n",
    "\n",
    "train = {'pos':0, 'neg':0}\n",
    "test = {'pos':0, 'neg':0}\n",
    "for text, verdict in training_set:\n",
    "    train[verdict] += 1\n",
    "print(train)\n",
    "\n",
    "for text, verdict in testing_set:\n",
    "    test[verdict] += 1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "Assumes that features are independent and is rather basic but tends to work well even when this assumption isn't entirely true.\n",
    "\n",
    "posterior = prior occurences x likelihood / evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.0\n",
      "Most Informative Features\n",
      "             fascination = True              pos : neg    =     10.1 : 1.0\n",
      "               addresses = True              pos : neg    =      9.4 : 1.0\n",
      "                  hudson = True              neg : pos    =      9.2 : 1.0\n",
      "              weaknesses = True              pos : neg    =      8.1 : 1.0\n",
      "              infectious = True              pos : neg    =      7.5 : 1.0\n",
      "               balancing = True              pos : neg    =      7.5 : 1.0\n",
      "               behaviour = True              pos : neg    =      7.5 : 1.0\n",
      "                  shoddy = True              neg : pos    =      6.5 : 1.0\n",
      "              annoyingly = True              neg : pos    =      6.5 : 1.0\n",
      "                  wasted = True              neg : pos    =      6.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(nltk.classify.accuracy(classifier, testing_set) * 100)\n",
    "print(classifier.show_most_informative_features())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Classifiers\n",
    "\n",
    "```\n",
    "# save\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "```\n",
    "\n",
    "```\n",
    "# load\n",
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "# multinomial = not binary distribution\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n"
     ]
    }
   ],
   "source": [
    "mnb_classifier = SklearnClassifier(MultinomialNB())\n",
    "mnb_classifier.train(training_set)\n",
    "\n",
    "print(nltk.classify.accuracy(mnb_classifier, testing_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n"
     ]
    }
   ],
   "source": [
    "bernoullinb_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoullinb_classifier.train(training_set)\n",
    "\n",
    "print(nltk.classify.accuracy(bernoullinb_classifier, testing_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.0\n"
     ]
    }
   ],
   "source": [
    "log_classifier = SklearnClassifier(LogisticRegression())\n",
    "log_classifier.train(training_set)\n",
    "\n",
    "print(nltk.classify.accuracy(log_classifier, testing_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.0\n"
     ]
    }
   ],
   "source": [
    "sgdc_classifier = SklearnClassifier(SGDClassifier())\n",
    "sgdc_classifier.train(training_set)\n",
    "\n",
    "print(nltk.classify.accuracy(sgdc_classifier, testing_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.0\n"
     ]
    }
   ],
   "source": [
    "svc_classifier = SklearnClassifier(SVC())\n",
    "svc_classifier.train(training_set)\n",
    "\n",
    "print(nltk.classify.accuracy(svc_classifier, testing_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.0\n"
     ]
    }
   ],
   "source": [
    "linear_svc_classifier = SklearnClassifier(LinearSVC())\n",
    "linear_svc_classifier.train(training_set)\n",
    "\n",
    "print(nltk.classify.accuracy(linear_svc_classifier, testing_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.0\n"
     ]
    }
   ],
   "source": [
    "nu_svc_classifier = SklearnClassifier(NuSVC())\n",
    "nu_svc_classifier.train(training_set)\n",
    "\n",
    "print(nltk.classify.accuracy(nu_svc_classifier, testing_set) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.0\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(classifier, mnb_classifier, bernoullinb_classifier, log_classifier, sgdc_classifier, svc_classifier, nu_svc_classifier)\n",
    "print((nltk.classify.accuracy(voted_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: pos Confidence %: 85.71428571428571\n",
      "Classification: neg Confidence %: 57.14285714285714\n",
      "Classification: pos Confidence %: 100.0\n",
      "Classification: neg Confidence %: 71.42857142857143\n",
      "Classification: pos Confidence %: 71.42857142857143\n",
      "Classification: pos Confidence %: 100.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Bias in Classifier\n",
    "Determine whether the classifer does better with different classes. Usually you'd generate a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NER Tagger\n",
    "\n",
    "Alternative to NLTK's named entity recognition classifier. It's regarded as the best but is slower. It provides multiple models for extracting named entities:\n",
    "\n",
    "- 3 class model for recognizing locations, persons, and organizations\n",
    "- 4 class model for recognizing locations, persons, organizations, and miscellaneous entities\n",
    "- 7 class model for recognizing locations, persons, organizations, times, money, percents, and dates\n",
    "\n",
    "The tagger is written in Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with NLTK and Gensim\n",
    "## Exploring\n",
    "\n",
    "- Tokens are not words: they're substrings and only structural while words are objects that have meaning\n",
    "- Concordance: Searches for text and provides the surrounding context\n",
    "- Similar: Can find words that occur frequently in the same context as a word. Allows you to do things like understand how words are being used in different texts\n",
    "- Common Contexts: Identify contexts for sets of words\n",
    "- Dispersion plot: can use to visualise frequency of words in different texts throughout time.\n",
    "- Stop Words: Eliminate common words - 'a', 'and', 'the', etc.\n",
    "\n",
    "## Frequency Analyses\n",
    "- Can count token frequency in text. NLTK comes with two useful classes:\n",
    "\t- `FreqDist`\n",
    "\t- `ConditionalFreqDist`\n",
    "- Words that occur infrequently or even only once are usually very important. Stop words can be useful when identifying these words.\n",
    "- We can compute:\n",
    "\t- The count of words\n",
    "\t- Vocabulary (unique words)\n",
    "\t- Lexical diversity (ratio of word count to vocabulary). Average number of times a word occurs in a corpus. It is useful for corpus analysis and can help inform you if the corpus has changed significantly under the hood. It can help you identify when you have problems in your analysis.\n",
    "- `most_common()` lets you retrieve tuples of the most common tokens and their counts\n",
    "- `counts.hapaxes()` tokens that occur only once\n",
    "- `counts.freq` how often a word occurs in the corpus\n",
    "- Conditional Frequency: Frequency of an event given a condition. \n",
    "\n",
    "## Features\n",
    "- Document Level Features:\n",
    "\t- Metadata: title, author\n",
    "\t- Paragraphs\n",
    "\t- Sentence construction\n",
    "- Word Level Features\n",
    "\t- Vocabulary\n",
    "\t- Form (Capitalization)\n",
    "\t- Frequency\n",
    "- Vector Encoding: Basic representation of documents: vector whose length is equal to the vocabulary of the entire corpus. Word positions in the vector are based on lexicographic (alphabetical) order\n",
    "- Bag of words: Token frequency is one of the simplest models - calculate the frequency of words in the document and use those numbers as the vector encoding. You can normalise the frequencies according to the total length of the vocabulary of the corpus. Useful for terms that occur frequently that are important. You need to remove stop words for this to be effective.\n",
    "- One hot encoding: Feature vector encodes vocabulary of document: words are equally distant - give 1 if present, 0 if not. Often used for neural network models.\n",
    "- TF-IDF Encoding: Highlight terms that are relevant to a document relative to the rest of the corpus by computing the term frequency times the inverse document frequency of the term.\n",
    "- Distributed representation: Can be used to encode similarity within vector space. Implemented in Gensim's doc2vec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
